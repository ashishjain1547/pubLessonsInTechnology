<head>
    <script crossorigin="anonymous" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
    
    <!-- Google AdSense Using Machine Learning Code -->
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-3071098372371409",
            enable_page_level_ads: true
        });
    </script>

    <style>
        .customTempCodeHolderForSocialMedia {
            display: none;
        }

        pre {
            white-space: pre-wrap;
            white-space: -moz-pre-wrap;
            white-space: -pre-wrap;
            white-space: -o-pre-wrap;
            word-wrap: break-word;
        }
    </style>
</head>

<pre>The simplest one is called 'Step function' or 'Threshold function':

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-dfRNLCstUg8/XwLI3ouAa-I/AAAAAAAAFHk/gJKjX2soFqIbmDQkgA_TSGlS3h-ToMm2wCK4BGAsYHg/s1097/Threshold.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="561" data-original-width="1097" height="328" src="https://1.bp.blogspot.com/-dfRNLCstUg8/XwLI3ouAa-I/AAAAAAAAFHk/gJKjX2soFqIbmDQkgA_TSGlS3h-ToMm2wCK4BGAsYHg/w640-h328/Threshold.png" width="640" /></a></div>

Second is the 'Rectifier function':

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-Zr-V5s-hpEI/XwLI2atAk6I/AAAAAAAAFHY/j0AtyXtJ2FcrDZoQ3hD9Le1SrHgX05UewCK4BGAsYHg/s971/Rectifier.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="553" data-original-width="971" height="364" src="https://1.bp.blogspot.com/-Zr-V5s-hpEI/XwLI2atAk6I/AAAAAAAAFHY/j0AtyXtJ2FcrDZoQ3hD9Le1SrHgX05UewCK4BGAsYHg/w640-h364/Rectifier.png" width="640" /></a></div>

Third is the Sigmoid function. We also see this one in 'logictic regression' algorithm.

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-HSWjYjsyVME/XwLI2qwXmcI/AAAAAAAAFHc/9psrdt5UZcIhN8LLPA2euFlblcXYrSZZwCK4BGAsYHg/s1007/Sigmoid.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="547" data-original-width="1007" height="348" src="https://1.bp.blogspot.com/-HSWjYjsyVME/XwLI2qwXmcI/AAAAAAAAFHc/9psrdt5UZcIhN8LLPA2euFlblcXYrSZZwCK4BGAsYHg/w640-h348/Sigmoid.png" width="640" /></a></div>

Fourth is Hyperbolic Tangent.

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-ljuAfKdD19A/XwLI19a1qcI/AAAAAAAAFHU/nkZJwjVh5fMuOXH2rmyNncqryKGcUZdhACK4BGAsYHg/s1061/Hyperbolic%2BTangent.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="549" data-original-width="1061" height="332" src="https://1.bp.blogspot.com/-ljuAfKdD19A/XwLI19a1qcI/AAAAAAAAFHU/nkZJwjVh5fMuOXH2rmyNncqryKGcUZdhACK4BGAsYHg/w640-h332/Hyperbolic%2BTangent.png" width="640" /></a></div>

Fifth is Softplus. Softplus function is a smooth version of the Rectifier function.

f(x) = log(1 + exp(x))

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-a7DSK7AjQKU/XwLI3K--P6I/AAAAAAAAFHg/Bm8l2WeMkOkUJxDZbsLYuLIVJwg1XG0fACK4BGAsYHg/s531/Softplus.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="449" data-original-width="531" height="542" src="https://1.bp.blogspot.com/-a7DSK7AjQKU/XwLI3K--P6I/AAAAAAAAFHg/Bm8l2WeMkOkUJxDZbsLYuLIVJwg1XG0fACK4BGAsYHg/w640-h542/Softplus.PNG" width="640" /></a></div>

Below is an illustrative image showing how multiple activation functions are used in a neural network. Here, one in hidden layers and one in output layer:

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-9RX26LpAYT8/XwLI1Q1VQvI/AAAAAAAAFHQ/oA0OeDhS4R0yP2yDREGw1G-Z1iO6sp4NACK4BGAsYHg/s615/How%2Bthey%2Bfit%2Btogether.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="318" data-original-width="615" height="331" src="https://1.bp.blogspot.com/-9RX26LpAYT8/XwLI1Q1VQvI/AAAAAAAAFHQ/oA0OeDhS4R0yP2yDREGw1G-Z1iO6sp4NACK4BGAsYHg/w640-h331/How%2Bthey%2Bfit%2Btogether.png" width="640" /></a></div>

A research paper proposes that Rectifier function gives a better performing neural network than one using 'Sigmoid' or 'Hyperbolic Tangent'.
<a href="https://drive.google.com/file/d/1OEOxeOMh7fG0eirbtt3ZcKr-Ijk4CzLT/view?usp=sharing" target="_blank">Deep Sparse Rectifier Neural Networks, Xavier Glorot, 2011</a>

Sixth: Softmax Function

It converts a vector of K real numbers into a probability distribution of K possible outcomes.

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgP7xnz9oKcl1hY3gbYqeiqtDbkLdRiRaXgOCVMsj0VaZyTPLaNgKK9FuKkViIafdRiS5JuSAZZO5rquXFrB0dxPchZPsTpzXrAsO2Q9rxa6GmedXyE_gehYFlQsi-rw_f6LBL-80s5uoGyHi6ugP0vP6DAtklziXSOjVp4BsUr5ICEuNgOM9MATAwMKw/s390/softmax%20equation.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" width="600" data-original-height="147" data-original-width="390" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgP7xnz9oKcl1hY3gbYqeiqtDbkLdRiRaXgOCVMsj0VaZyTPLaNgKK9FuKkViIafdRiS5JuSAZZO5rquXFrB0dxPchZPsTpzXrAsO2Q9rxa6GmedXyE_gehYFlQsi-rw_f6LBL-80s5uoGyHi6ugP0vP6DAtklziXSOjVp4BsUr5ICEuNgOM9MATAwMKw/s600/softmax%20equation.png"/></a></div>

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCKvCrVVME53iL7X1Gq_H-AlI72cifv83Q1V6QHP9b2RkkXJrH2LeahxPmWWtmnyZSI5iVt7cBi7lcaEXUiZRPKV2SsqNHTvlBr_ZpEXXuSnHhvYWMOLq7q0lZKFOCfudnfmUQsS3iEPK4lSz_9TFrIcwQQt2hfOnoPaIouJVv4YFg0YZwiYzt22QUlA/s953/softmax.jpeg" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" width="600" data-original-height="490" data-original-width="953" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCKvCrVVME53iL7X1Gq_H-AlI72cifv83Q1V6QHP9b2RkkXJrH2LeahxPmWWtmnyZSI5iVt7cBi7lcaEXUiZRPKV2SsqNHTvlBr_ZpEXXuSnHhvYWMOLq7q0lZKFOCfudnfmUQsS3iEPK4lSz_9TFrIcwQQt2hfOnoPaIouJVv4YFg0YZwiYzt22QUlA/s600/softmax.jpeg"/></a></div>

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgx8VumI1YterlkVJcRbY7ZNWWI8iJ3E3YkQ9OlHxrzBEOolCWYyAGn5DYJNFySquHPLSZwIqSC7NDoRvweC4bhwfOEMiG-l6graw9-BBkov0wkqVo4k1wtcA6wafMgPSB90vpcEC6khriy0DKhPNdqqWluWpHFW4n1bzyrEEbHdWOg4F6sX7t-bU3t4g/s1340/Softmax-Function.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" width="600" data-original-height="680" data-original-width="1340" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgx8VumI1YterlkVJcRbY7ZNWWI8iJ3E3YkQ9OlHxrzBEOolCWYyAGn5DYJNFySquHPLSZwIqSC7NDoRvweC4bhwfOEMiG-l6graw9-BBkov0wkqVo4k1wtcA6wafMgPSB90vpcEC6khriy0DKhPNdqqWluWpHFW4n1bzyrEEbHdWOg4F6sX7t-bU3t4g/s600/Softmax-Function.png"/></a></div>

Statisticians usually call softmax a "multiple logistic" function. It reduces to the simple logistic function when there are only two categories.
The first category is q_1. And suppose you choose to set q_2 to 0. Then 

           exp(q_1)         exp(q_1)              1
   p_1 = ------------ = ----------------- = -------------
          c             exp(q_1) + exp(0)   1 + exp(-q_1)
         sum exp(q_j)
         j=1

and p_2, of course, is 1 - p_1. 

Ref: <a href="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html" target="_blank">ai-faq/neural-nets</a> 
</pre>
[
  {
    "question": "According to the text, what is a common source of training data for many foundation models, known for its questionable quality?",
    "options": [
      "Wikipedia",
      "Common Crawl",
      "Reddit",
      "Project Gutenberg"
    ],
    "answer": [
      "Common Crawl"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What is the primary goal of the post-training process for a foundation model?",
    "options": [
      "To increase the number of parameters.",
      "To align the model with human preferences.",
      "To pre-train the model on a larger dataset.",
      "To change the model's architecture."
    ],
    "answer": [
      "To align the model with human preferences."
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "Which model architecture is currently the most dominant for language-based foundation models?",
    "options": [
      "Recurrent Neural Network (RNN)",
      "Seq2seq",
      "Transformer",
      "State Space Model (SSM)"
    ],
    "answer": [
      "Transformer"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "In the context of sampling, what does a lower temperature value (e.g., close to 0) generally lead to?",
    "options": [
      "More creative and random responses.",
      "More consistent and predictable responses.",
      "Shorter responses.",
      "Longer responses."
    ],
    "answer": [
      "More consistent and predictable responses."
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What is the term for when a model provides a response that is not grounded in facts?",
    "options": [
      "Inconsistency",
      "Hallucination",
      "Sampling error",
      "Overfitting"
    ],
    "answer": [
      "Hallucination"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What are the two steps that post-training generally consists of?",
    "options": [
      "Pre-training and Self-supervision",
      "Supervised finetuning (SFT) and Preference finetuning",
      "Scaling and Sampling",
      "Tokenization and Embedding"
    ],
    "answer": [
      "Supervised finetuning (SFT) and Preference finetuning"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What does 'SFT' stand for in the context of model training?",
    "options": [
      "Standard Finetuning Technique",
      "Supervised Finetuning",
      "Systematic Fault Tolerance",
      "Sequential Forward Transfer"
    ],
    "answer": [
      "Supervised Finetuning"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "Which language dominates the Common Crawl dataset, accounting for almost half of the data?",
    "options": [
      "Chinese",
      "Russian",
      "Spanish",
      "English"
    ],
    "answer": [
      "English"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What type of sampling strategy always picks the outcome with the highest probability?",
    "options": [
      "Top-p sampling",
      "Top-k sampling",
      "Greedy sampling",
      "Temperature sampling"
    ],
    "answer": [
      "Greedy sampling"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What is the term for a model that has a large percentage of zero-value parameters?",
    "options": [
      "Dense model",
      "Sparse model",
      "Compact model",
      "Optimized model"
    ],
    "answer": [
      "Sparse model"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "A model's inference cost and latency are proportional to the number of what?",
    "options": [
      "Parameters",
      "Layers",
      "Tokens",
      "Training hours"
    ],
    "answer": [
      "Tokens"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What does 'RLHF' stand for?",
    "options": [
      "Reinforcement Learning with Human Feedback",
      "Recurrent Learning from Human Features",
      "Reinforcement Learning from Human Feedback",
      "Recursive Learning with Human Functions"
    ],
    "answer": [
      "Reinforcement Learning from Human Feedback"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "Which of the following is NOT a sampling variable mentioned in the text?",
    "options": [
      "Temperature",
      "Top-k",
      "Top-p",
      "Bottom-n"
    ],
    "answer": [
      "Bottom-n"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What is the purpose of a reward model in RLHF?",
    "options": [
      "To generate the final response.",
      "To score the foundation model's outputs based on human preference.",
      "To pre-train the initial model.",
      "To summarize the input prompt."
    ],
    "answer": [
      "To score the foundation model's outputs based on human preference."
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What does MoE stand for in the context of sparse models?",
    "options": [
      "Model of Experts",
      "Mixture-of-Experts",
      "Margin of Error",
      "Multiple-output Emitter"
    ],
    "answer": [
      "Mixture-of-Experts"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "The text states that a model's outputs are probabilistic. What is the opposite of probabilistic?",
    "options": [
      "Stochastic",
      "Randomized",
      "Deterministic",
      "Variable"
    ],
    "answer": [
      "Deterministic"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "Which of the following are the 'three golden goals for training data'?",
    "options": [
      "Size, Speed, and Source",
      "Quantity, Quality, and Diversity",
      "Cost, Availability, and Language",
      "Format, Structure, and Accuracy"
    ],
    "answer": [
      "Quantity, Quality, and Diversity"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What is a parameter that is set by users to configure a model and control how it learns called?",
    "options": [
      "Weight",
      "Bias",
      "Hyperparameter",
      "Neuron"
    ],
    "answer": [
      "Hyperparameter"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "What is the technique called where you generate multiple outputs for a single query to increase the chance of a good response?",
    "options": [
      "Greedy sampling",
      "Model finetuning",
      "Test time compute",
      "Constrained sampling"
    ],
    "answer": [
      "Test time compute"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "The text mentions two visible bottlenecks for scaling foundation models further. What are they?",
    "options": [
      "Algorithm complexity and memory speed",
      "Training data and electricity",
      "GPU availability and network bandwidth",
      "Software limitations and cooling systems"
    ],
    "answer": [
      "Training data and electricity"
    ],
    "type": "scq",
    "complexity": "easy"
  },
  {
    "question": "The Transformer architecture uses an attention mechanism. What are the three vector types it leverages to weigh the importance of input tokens?",
    "options": [
      "Start, Middle, and End vectors",
      "Input, Output, and Hidden vectors",
      "Past, Present, and Future vectors",
      "Query, Key, and Value vectors"
    ],
    "answer": [
      "Query, Key, and Value vectors"
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "What does the Chinchilla scaling law suggest for compute-optimal training regarding the relationship between model size and the number of training tokens?",
    "options": [
      "Model size should be 100 times the number of training tokens.",
      "The number of training tokens should be approximately 20 times the model size.",
      "Model size and training tokens should be increased independently without a fixed ratio.",
      "For every doubling of model size, the number of training tokens should be halved."
    ],
    "answer": [
      "The number of training tokens should be approximately 20 times the model size."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "What were the two primary problems with the vanilla seq2seq (RNN-based) architecture that the Transformer architecture was designed to solve?",
    "options": [
      "It was too large and consumed too much electricity.",
      "It could only process English and was not good at translation.",
      "The decoder used only the final hidden state of the input, and its sequential nature was slow for long sequences.",
      "It lacked an attention mechanism entirely and could not handle variable-length inputs."
    ],
    "answer": [
      "The decoder used only the final hidden state of the input, and its sequential nature was slow for long sequences."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "The text describes two hypotheses for why language models hallucinate. Which of the following accurately summarizes them?",
    "options": [
      "1) Hallucination is caused by malicious data injection, and 2) It is a result of hardware errors during inference.",
      "1) The model can't differentiate between given data and its own generated data (self-delusion), and 2) There is a mismatch between the model's internal knowledge and the labeler's knowledge used during SFT.",
      "1) The temperature setting is too high during sampling, and 2) The model's vocabulary is too small to represent facts accurately.",
      "1) It is an emergent property of all large neural networks, and 2) It is a side effect of using the softmax function for probability distribution."
    ],
    "answer": [
      "1) The model can't differentiate between given data and its own generated data (self-delusion), and 2) There is a mismatch between the model's internal knowledge and the labeler's knowledge used during SFT."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "How does top-p (nucleus) sampling differ from top-k sampling?",
    "options": [
      "Top-p considers a fixed number of tokens, while top-k considers a dynamic number.",
      "Top-p allows for a dynamic selection of tokens to sample from based on a cumulative probability threshold, whereas top-k considers a fixed number of the most likely tokens.",
      "Top-p is used for classification tasks, while top-k is used for generative tasks.",
      "Top-p always results in more creative outputs, while top-k always results in more factual outputs."
    ],
    "answer": [
      "Top-p allows for a dynamic selection of tokens to sample from based on a cumulative probability threshold, whereas top-k considers a fixed number of the most likely tokens."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "According to the text, why is it preferable to use 'comparison data' (e.g., response A is better than B) instead of 'pointwise evaluation' (e.g., response A gets a score of 7/10) for training a reward model?",
    "options": [
      "Comparison data is faster to generate and leads to more consistent scoring from human labelers.",
      "Pointwise evaluation requires a larger model to process.",
      "Comparison data can be generated by AI, while pointwise evaluation requires humans.",
      "Pointwise evaluation is mathematically incompatible with reinforcement learning algorithms like PPO."
    ],
    "answer": [
      "Comparison data is faster to generate and leads to more consistent scoring from human labelers."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "What is the primary difference in the optimization goals between pre-training and post-training for language models?",
    "options": [
      "Pre-training optimizes for speed, while post-training optimizes for accuracy.",
      "Pre-training optimizes for a small model size, while post-training optimizes for a large model size.",
      "Pre-training optimizes for token-level quality (e.g., predicting the next token), while post-training optimizes for response-level quality that aligns with user preference.",
      "Pre-training uses supervised data, while post-training uses unsupervised data."
    ],
    "answer": [
      "Pre-training optimizes for token-level quality (e.g., predicting the next token), while post-training optimizes for response-level quality that aligns with user preference."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "In the context of Mixture-of-Experts (MoE) models like Mixtral 8x7B, why is the inference cost and speed equivalent to a much smaller dense model?",
    "options": [
      "All experts process every token, but do so in parallel.",
      "Only a subset of the experts is active to process each token at each layer.",
      "The parameters are compressed during inference to a smaller size.",
      "MoE models use a different, more efficient architecture than transformers."
    ],
    "answer": [
      "Only a subset of the experts is active to process each token at each layer."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "The text describes how 'constrained sampling' can be used to generate structured outputs. How does this technique work at a high level?",
    "options": [
      "It generates many outputs and uses another AI model to select the one with the correct structure.",
      "It finetunes the model on a large dataset of structured examples until it learns the format.",
      "At each token generation step, it filters the model's logits to only consider tokens that are valid according to a predefined grammar (e.g., JSON grammar).",
      "It uses post-processing scripts to fix common formatting errors after the model generates a response."
    ],
    "answer": [
      "At each token generation step, it filters the model's logits to only consider tokens that are valid according to a predefined grammar (e.g., JSON grammar)."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "What are some of the potential negative consequences of recursively training new AI models on AI-generated data from the internet?",
    "options": [
      "The models will become more creative and less prone to bias.",
      "The models may gradually forget original data patterns, leading to degraded performance over time.",
      "The models will require significantly less training data and compute power.",
      "The models will develop a perfect understanding of human preferences."
    ],
    "answer": [
      "The models may gradually forget original data patterns, leading to degraded performance over time."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "The document shows GPT-4 performs worse on languages like Telugu and Punjabi on the MMLU benchmark. Besides under-representation in training data, what is another reason cited for this underperformance?",
    "options": [
      "These languages use characters that are difficult for the model to tokenize.",
      "The MMLU benchmark was poorly translated into these languages.",
      "The language's structure and the culture it embodies can make it harder for a model to learn.",
      "Users of these languages provide less feedback data for preference tuning."
    ],
    "answer": [
      "The language's structure and the culture it embodies can make it harder for a model to learn."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "The use of a verifier model was found to provide a performance boost equivalent to a 30x increase in model size. How does a verifier achieve this?",
    "options": [
      "It rewrites the prompts to be easier for the main model.",
      "It helps the main model sample multiple outputs and then selects the best one.",
      "It adds more parameters to the main model during inference.",
      "It compresses the output to reduce token count."
    ],
    "answer": [
      "It helps the main model sample multiple outputs and then selects the best one."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "What is 'scaling extrapolation' in the context of training large models?",
    "options": [
      "Increasing a model's context length after it has been trained.",
      "Predicting the best-performing hyperparameters for large models by studying their impact on smaller models.",
      "Using a small model to generate synthetic data for training a larger model.",
      "A law that states model performance will always increase with size."
    ],
    "answer": [
      "Predicting the best-performing hyperparameters for large models by studying their impact on smaller models."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "The Shoggoth with a smiley face meme is used as an analogy for the foundation model training process. Match the training steps to the parts of the analogy.",
    "options": [
      "Self-supervised pre-training is the smiley face; SFT and RLHF are the untamed monster.",
      "Self-supervised pre-training is the untamed monster; SFT makes it socially acceptable; RLHF adds the customer-appropriate smiley face.",
      "Self-supervised pre-training is giving it a smiley face; RLHF is the untamed monster.",
      "All three steps (pre-training, SFT, RLHF) contribute equally to creating the untamed monster."
    ],
    "answer": [
      "Self-supervised pre-training is the untamed monster; SFT makes it socially acceptable; RLHF adds the customer-appropriate smiley face."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "The text discusses several approaches to get models to generate structured outputs. Which of the following approaches are mentioned? (Select all that apply)",
    "options": [
      "Prompting",
      "Post-processing",
      "Constrained sampling",
      "Finetuning"
    ],
    "answer": [
      "Prompting",
      "Post-processing",
      "Constrained sampling",
      "Finetuning"
    ],
    "type": "mcq",
    "complexity": "complex"
  },
  {
    "question": "Why can tokenization be significantly less efficient for languages like Burmese or Hindi compared to English, and what are the direct consequences of this inefficiency?",
    "options": [
      "These languages have more words, so the vocabulary is larger, which has no consequence on cost.",
      "Their characters are more complex, requiring multiple tokens to represent what would be a single token in English, leading to higher costs and longer generation times.",
      "The models are not trained on these languages, so they default to a less efficient tokenization scheme, which only affects model accuracy.",
      "They use a different byte-encoding standard, which is slower to process but does not increase costs."
    ],
    "answer": [
      "Their characters are more complex, requiring multiple tokens to represent what would be a single token in English, leading to higher costs and longer generation times."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "What is the 'inverse scaling' phenomenon?",
    "options": [
      "A situation where a model's performance on a specific task gets worse as the model's size increases.",
      "When a smaller model outperforms a larger model due to more efficient architecture.",
      "The process of reducing a model's size after training to make it more efficient.",
      "The observation that training costs decrease as model sizes increase."
    ],
    "answer": [
      "A situation where a model's performance on a specific task gets worse as the model's size increases."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "A Transformer block generally contains which two main modules?",
    "options": [
      "An RNN module and a Convolutional module",
      "An Embedding module and an Output module",
      "An Attention module and an MLP (multi-layer perceptron) module",
      "A Tokenizer module and a Sampling module"
    ],
    "answer": [
      "An Attention module and an MLP (multi-layer perceptron) module"
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "The document discusses alternative architectures to the Transformer, such as Mamba and Jamba. What is a key theoretical advantage of these RNN/SSM-based models over Transformers?",
    "options": [
      "They require significantly more parameters to achieve the same performance.",
      "Their inference computation scales linearly with sequence length, whereas Transformers scale quadratically.",
      "They are only effective for non-English languages.",
      "They cannot be parallelized during training, making them slower but more accurate."
    ],
    "answer": [
      "Their inference computation scales linearly with sequence length, whereas Transformers scale quadratically."
    ],
    "type": "scq",
    "complexity": "complex"
  },
  {
    "question": "Why is it important to consider the average logprob, rather than the total sum of logprobs, when selecting the best output from multiple generated sequences?",
    "options": [
      "Because the total sum of logprobs is always positive, making it difficult to compare.",
      "To avoid biasing the selection towards shorter sequences, as longer sequences naturally accumulate a lower (more negative) total logprob.",
      "Because calculating the average is computationally cheaper than calculating the sum.",
      "To ensure that the selected sequence has the highest possible creativity score."
    ],
    "answer": [
      "To avoid biasing the selection towards shorter sequences, as longer sequences naturally accumulate a lower (more negative) total logprob."
    ],
    "type": "scq",
    "complexity": "complex"
  }
]
<head>
    <script crossorigin="anonymous" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        src="https://code.jquery.com/jquery-3.2.1.min.js"></script>

    <!-- Google AdSense Using Machine Learning Code -->
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-3071098372371409",
            enable_page_level_ads: true
        });
    </script>

    <script>
        $(document).ready(function () {
            $.ajax({
                url: "https://raw.githubusercontent.com/ashishjain1547/bookSummariesAndReviews/main/links_to_book_clubs.json",
                success: function (result) {
                    let grouplink = JSON.parse(result)['current book club'];
                    $("#customWhatsAppGroupLinkWrapper").html(
                        `
                        <h2 class="custom_link_h2"><a href="${grouplink}" target="_blank"> 
                            <span>Join us on:</span>
                            <span class="customLink"><i class="fa fa-whatsapp"></i> Whatsapp </span>
                            </a>
                        </h2>
                        `
                    );
                }
            });
        });
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <style>
        pre {
            white-space: pre-wrap;
            white-space: -moz-pre-wrap;
            white-space: -pre-wrap;
            white-space: -o-pre-wrap;
            word-wrap: break-word;
        }

        .customLink {
            background-color: #4CAF50;
            border: none;
            color: white !important;
            padding: 8px 13px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 14px;
            margin: 4px 2px;
            cursor: pointer;
        }

        .customLink:hover {
            text-decoration: none;
        }

        div.code-block-decoration.footer {
            display: none;
        }

        button.export-sheets-button-wrapper {
            display: none;
        }
    </style>

    <style>
        .custom_link_h2 a {
            color: black;
            text-decoration: none;
            text-align: center;
        }

        .custom_link_h2 a:hover {
            color: black;
        }

        .custom_link_h2 a:active {
            color: black;
        }

        .custom_link_h2 span {
            translate: 0px -5px;
            display: inline-block;
        }

        .custom_link_h2 img {
            width: 100px;
            padding: 0px;
            border: none;
            box-shadow: none;
        }

        .customul {
            list-style: none;
        }

        [aria-hidden='true'] {
            display: none;
        }
    </style>

    <style>
        pre {
            white-space: pre-wrap;
            white-space: -moz-pre-wrap;
            white-space: -pre-wrap;
            white-space: -o-pre-wrap;
            word-wrap: break-word;
        }

        .dot {
            height: 12px;
            width: 12px;
            background-color: #bbb;
            border-radius: 50%;
            display: inline-block;
        }

        .arrow {
            border: solid black;
            border-width: 0 3px 3px 0;
            display: inline-block;
            padding: 3px;
        }

        .right {
            transform: rotate(-45deg);
            -webkit-transform: rotate(-45deg);
        }

        .left {
            transform: rotate(135deg);
            -webkit-transform: rotate(135deg);
        }

        .up {
            transform: rotate(-135deg);
            -webkit-transform: rotate(-135deg);
        }

        .down {
            transform: rotate(45deg);
            -webkit-transform: rotate(45deg);
        }
    </style>

    <style>
        span.relative.inline-flex.items-center button {
            display: none;
        }

        div.bg-token-bg-elevated-secondary.text-token-text-secondary.flex button {
            display: none;
        }
    </style>
</head>

<a class="customLink" href="https://github.com/ashishjain1547/agentic_ai_books/blob/main/1_Chip%20Huyen%20-%20AI%20Engineering_%20Building%20Applications%20with%20Foundation%20Models-O'Reilly%20Media%20(2025).pdf" target="_blank">Download Book</a>
<br><br>
<div id="customWhatsAppGroupLinkWrapper"></div>
<a class="customLink" href="https://survival8.blogspot.com/2025/12/chapter-8-ai-engineering-by-chip-huyen.html" target="_blank">&lt;&lt;&lt; Previous Chapter</a>
<a class="customLink" href="https://survival8.blogspot.com/2025/12/chapter-10-ai-engineering-by-chip-huyen.html" target="_blank">Next Chapter &gt;&gt;&gt;</a>
<br><br>

<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigCuANDr8rihCNvkpsxyFUOemO9pcK6MJAaHe9AaR7V1m766NznPyvEWLWtLLfnk8ZRI4NjQkHLnhO9ljovbMsHqpg8IaKAnJ1F4nBc3jxkAzc7p3Prue1meNsPkXMBe19-p_Kmq23Hz4oPFB6x3qYWqMSb6ieGviU8Jcl24a3T1Z9EHfRs4neLXlbnN6m/s686/Screenshot%20from%202025-06-26%2009-45-44.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" height="600" data-original-height="686" data-original-width="546" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigCuANDr8rihCNvkpsxyFUOemO9pcK6MJAaHe9AaR7V1m766NznPyvEWLWtLLfnk8ZRI4NjQkHLnhO9ljovbMsHqpg8IaKAnJ1F4nBc3jxkAzc7p3Prue1meNsPkXMBe19-p_Kmq23Hz4oPFB6x3qYWqMSb6ieGviU8Jcl24a3T1Z9EHfRs4neLXlbnN6m/s600/Screenshot%20from%202025-06-26%2009-45-44.png"/></a></div>



<div class="markdown prose dark:prose-invert w-full break-words light markdown-new-styling">


<p data-start="495" data-end="558"><em data-start="495" data-end="558">Why “running” an AI model well is just as hard as building it</em></p>
<hr data-start="560" data-end="563">
<h2 data-start="565" data-end="623">Introduction: Why Inference Matters More Than You Think</h2>
<p data-start="625" data-end="829">In the AI world, we spend a lot of time talking about training models—bigger models, better architectures, more data, and more GPUs. But once a model is trained, a much more practical question takes over:</p>
<p data-start="831" data-end="880"><strong data-start="831" data-end="880">How do you run it efficiently for real users?</strong></p>
<p data-start="882" data-end="932">This is where <strong data-start="896" data-end="922">inference optimization</strong> comes in.</p>
<p data-start="934" data-end="1035">Inference is the process of using a trained model to produce outputs for new inputs. In simple terms:</p>
<ul data-start="1036" data-end="1092">
<li data-start="1036" data-end="1060">
<p data-start="1038" data-end="1060">Training is <em data-start="1050" data-end="1060">learning</em></p>
</li>
<li data-start="1061" data-end="1092">
<p data-start="1063" data-end="1092">Inference is <em data-start="1076" data-end="1092">doing the work</em></p>
</li>
</ul>
<p data-start="1094" data-end="1379">No matter how brilliant a model is, if it’s too slow, users will abandon it. If it’s too expensive, businesses won’t deploy it. Worse, if inference takes longer than the value of the prediction, the model becomes useless—imagine a stock prediction that arrives after the market closes.</p>
<p data-start="1381" data-end="1581">As the chapter makes clear, <strong data-start="1409" data-end="1510">inference optimization is about making AI models faster and cheaper without ruining their quality</strong>. And it’s not just a single discipline—it sits at the intersection of:</p>
<ul data-start="1582" data-end="1703">
<li data-start="1582" data-end="1600">
<p data-start="1584" data-end="1600">Machine learning</p>
</li>
<li data-start="1601" data-end="1622">
<p data-start="1603" data-end="1622">Systems engineering</p>
</li>
<li data-start="1623" data-end="1646">
<p data-start="1625" data-end="1646">Hardware architecture</p>
</li>
<li data-start="1647" data-end="1658">
<p data-start="1649" data-end="1658">Compilers</p>
</li>
<li data-start="1659" data-end="1680">
<p data-start="1661" data-end="1680">Distributed systems</p>
</li>
<li data-start="1681" data-end="1703">
<p data-start="1683" data-end="1703">Cloud infrastructure</p>
</li>
</ul>
<p data-start="1705" data-end="1952">This blog post explains inference optimization in plain language, without skipping the important details, so you can understand <em data-start="1833" data-end="1838">why</em> AI systems behave the way they do—and how teams improve them in production <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="1954" data-end="1957">
<h2 data-start="1959" data-end="2008">1. Understanding Inference: From Model to User</h2>
<h3 data-start="2010" data-end="2059">Training vs Inference (A Crucial Distinction)</h3>
<p data-start="2061" data-end="2105">Every AI model has two distinct life phases:</p>
<ol data-start="2106" data-end="2204">
<li data-start="2106" data-end="2151">
<p data-start="2109" data-end="2151"><strong data-start="2109" data-end="2121">Training</strong> – learning patterns from data</p>
</li>
<li data-start="2152" data-end="2204">
<p data-start="2155" data-end="2204"><strong data-start="2155" data-end="2168">Inference</strong> – generating outputs for new inputs</p>
</li>
</ol>
<p data-start="2206" data-end="2283">Training is expensive but happens infrequently. Inference happens constantly.</p>
<p data-start="2285" data-end="2471">Most AI engineers, application developers, and product teams spend <strong data-start="2352" data-end="2408">far more time worrying about inference than training</strong>, especially if they’re using pretrained or open-source models.</p>
<hr data-start="2473" data-end="2476">
<h3 data-start="2478" data-end="2511">What Is an Inference Service?</h3>
<p data-start="2513" data-end="2624">In production, inference doesn’t happen in isolation. It’s handled by an <strong data-start="2586" data-end="2607">inference service</strong>, which includes:</p>
<ul data-start="2625" data-end="2749">
<li data-start="2625" data-end="2670">
<p data-start="2627" data-end="2670">An <strong data-start="2630" data-end="2650">inference server</strong> that runs the model</p>
</li>
<li data-start="2671" data-end="2686">
<p data-start="2673" data-end="2686">Routing logic</p>
</li>
<li data-start="2687" data-end="2705">
<p data-start="2689" data-end="2705">Request handling</p>
</li>
<li data-start="2706" data-end="2749">
<p data-start="2708" data-end="2749">Possibly preprocessing and postprocessing</p>
</li>
</ul>
<p data-start="2751" data-end="3041">Model APIs like OpenAI, Gemini, or Claude are inference services. If you use them, you don’t worry about optimization details. But if you host your own models, <strong data-start="2911" data-end="2952">you own the entire inference pipeline</strong>—performance, cost, scaling, and failures included <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="3043" data-end="3046">
<h3 data-start="3048" data-end="3089">Why Optimization Is About Bottlenecks</h3>
<p data-start="3091" data-end="3134">Optimization always starts with a question:</p>
<blockquote data-start="3136" data-end="3164">
<p data-start="3138" data-end="3164"><em data-start="3138" data-end="3164">What is slowing us down?</em></p>
</blockquote>
<p data-start="3166" data-end="3327">Just like traffic congestion in a city, inference systems have chokepoints. Identifying these bottlenecks determines which optimization techniques actually help.</p>
<hr data-start="3329" data-end="3332">
<h2 data-start="3334" data-end="3398">2. Compute-Bound vs Memory-Bound: The Core Bottleneck Concept</h2>
<h3 data-start="3400" data-end="3431">Two Fundamental Bottlenecks</h3>
<p data-start="3433" data-end="3493">Inference workloads usually fall into one of two categories:</p>
<h4 data-start="3495" data-end="3513">Compute-bound</h4>
<ul data-start="3514" data-end="3661">
<li data-start="3514" data-end="3579">
<p data-start="3516" data-end="3579">Speed limited by how many calculations the hardware can perform</p>
</li>
<li data-start="3580" data-end="3621">
<p data-start="3582" data-end="3621">Example: heavy mathematical computation</p>
</li>
<li data-start="3622" data-end="3661">
<p data-start="3624" data-end="3661">Faster chips or more parallelism help</p>
</li>
</ul>
<h4 data-start="3663" data-end="3690">Memory bandwidth-bound</h4>
<ul data-start="3691" data-end="3841">
<li data-start="3691" data-end="3736">
<p data-start="3693" data-end="3736">Speed limited by how fast data can be moved</p>
</li>
<li data-start="3737" data-end="3801">
<p data-start="3739" data-end="3801">Common in large models where weights must be repeatedly loaded</p>
</li>
<li data-start="3802" data-end="3841">
<p data-start="3804" data-end="3841">Faster memory and smaller models help</p>
</li>
</ul>
<p data-start="3843" data-end="3954">This distinction is foundational to understanding inference optimization <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="3956" data-end="3959">
<h3 data-start="3961" data-end="4007">Why Language Models Are Often Memory-Bound</h3>
<p data-start="4009" data-end="4081">Large language models generate text one token at a time. For each token:</p>
<ul data-start="4082" data-end="4181">
<li data-start="4082" data-end="4125">
<p data-start="4084" data-end="4125">The model must load large weight matrices</p>
</li>
<li data-start="4126" data-end="4181">
<p data-start="4128" data-end="4181">Perform relatively little computation per byte loaded</p>
</li>
</ul>
<p data-start="4183" data-end="4249">This makes <strong data-start="4194" data-end="4206">decoding</strong> (token generation) memory bandwidth-bound.</p>
<hr data-start="4251" data-end="4254">
<h3 data-start="4256" data-end="4304">Prefill vs Decode: Two Very Different Phases</h3>
<p data-start="4306" data-end="4364">Transformer-based language model inference has two stages:</p>
<ol data-start="4366" data-end="4630">
<li data-start="4366" data-end="4499">
<p data-start="4369" data-end="4380"><strong data-start="4369" data-end="4380">Prefill</strong></p>
<ul data-start="4384" data-end="4499">
<li data-start="4384" data-end="4420">
<p data-start="4386" data-end="4420">Processes input tokens in parallel</p>
</li>
<li data-start="4424" data-end="4439">
<p data-start="4426" data-end="4439">Compute-bound</p>
</li>
<li data-start="4443" data-end="4499">
<p data-start="4445" data-end="4499">Determines how fast the model “understands” the prompt</p>
</li>
</ul>
</li>
<li data-start="4501" data-end="4630">
<p data-start="4504" data-end="4514"><strong data-start="4504" data-end="4514">Decode</strong></p>
<ul data-start="4518" data-end="4630">
<li data-start="4518" data-end="4556">
<p data-start="4520" data-end="4556">Generates one output token at a time</p>
</li>
<li data-start="4560" data-end="4584">
<p data-start="4562" data-end="4584">Memory bandwidth-bound</p>
</li>
<li data-start="4588" data-end="4630">
<p data-start="4590" data-end="4630">Determines how fast the response appears</p>
</li>
</ul>
</li>
</ol>
<p data-start="4632" data-end="4798">Because these phases behave differently, modern inference systems often <strong data-start="4704" data-end="4737">separate them across machines</strong> for better efficiency <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="4800" data-end="4803">
<h2 data-start="4805" data-end="4853">3. Online vs Batch Inference: Latency vs Cost</h2>
<h3 data-start="4855" data-end="4886">Two Types of Inference APIs</h3>
<p data-start="4888" data-end="4909">Most providers offer:</p>
<ul data-start="4910" data-end="5008">
<li data-start="4910" data-end="4955">
<p data-start="4912" data-end="4955"><strong data-start="4912" data-end="4927">Online APIs</strong> – optimized for low latency</p>
</li>
<li data-start="4956" data-end="5008">
<p data-start="4958" data-end="5008"><strong data-start="4958" data-end="4972">Batch APIs</strong> – optimized for cost and throughput</p>
</li>
</ul>
<hr data-start="5010" data-end="5013">
<h3 data-start="5015" data-end="5035">Online Inference</h3>
<p data-start="5037" data-end="5046">Used for:</p>
<ul data-start="5047" data-end="5100">
<li data-start="5047" data-end="5057">
<p data-start="5049" data-end="5057">Chatbots</p>
</li>
<li data-start="5058" data-end="5075">
<p data-start="5060" data-end="5075">Code assistants</p>
</li>
<li data-start="5076" data-end="5100">
<p data-start="5078" data-end="5100">Real-time interactions</p>
</li>
</ul>
<p data-start="5102" data-end="5118">Characteristics:</p>
<ul data-start="5119" data-end="5183">
<li data-start="5119" data-end="5132">
<p data-start="5121" data-end="5132">Low latency</p>
</li>
<li data-start="5133" data-end="5164">
<p data-start="5135" data-end="5164">Users expect instant feedback</p>
</li>
<li data-start="5165" data-end="5183">
<p data-start="5167" data-end="5183">Limited batching</p>
</li>
</ul>
<p data-start="5185" data-end="5328">Streaming responses (token-by-token) reduce perceived waiting time but come with risks—users might see bad outputs before they can be filtered.</p>
<hr data-start="5330" data-end="5333">
<h3 data-start="5335" data-end="5354">Batch Inference</h3>
<p data-start="5356" data-end="5365">Used for:</p>
<ul data-start="5366" data-end="5451">
<li data-start="5366" data-end="5393">
<p data-start="5368" data-end="5393">Synthetic data generation</p>
</li>
<li data-start="5394" data-end="5412">
<p data-start="5396" data-end="5412">Periodic reports</p>
</li>
<li data-start="5413" data-end="5434">
<p data-start="5415" data-end="5434">Document processing</p>
</li>
<li data-start="5435" data-end="5451">
<p data-start="5437" data-end="5451">Data migration</p>
</li>
</ul>
<p data-start="5453" data-end="5469">Characteristics:</p>
<ul data-start="5470" data-end="5555">
<li data-start="5470" data-end="5494">
<p data-start="5472" data-end="5494">Higher latency allowed</p>
</li>
<li data-start="5495" data-end="5516">
<p data-start="5497" data-end="5516">Aggressive batching</p>
</li>
<li data-start="5517" data-end="5555">
<p data-start="5519" data-end="5555">Much lower cost (often ~50% cheaper)</p>
</li>
</ul>
<p data-start="5557" data-end="5724">Unlike traditional ML, batch inference for foundation models can’t precompute everything because <strong data-start="5654" data-end="5685">user prompts are open-ended</strong> <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="5726" data-end="5729">
<h2 data-start="5731" data-end="5798">4. Measuring Inference Performance: Metrics That Actually Matter</h2>
<h3 data-start="5800" data-end="5829">Latency Is Not One Number</h3>
<p data-start="5831" data-end="5878">Latency is best understood as multiple metrics:</p>
<h4 data-start="5880" data-end="5911">Time to First Token (TTFT)</h4>
<ul data-start="5912" data-end="6007">
<li data-start="5912" data-end="5958">
<p data-start="5914" data-end="5958">How long users wait before seeing <em data-start="5948" data-end="5958">anything</em></p>
</li>
<li data-start="5959" data-end="5976">
<p data-start="5961" data-end="5976">Tied to prefill</p>
</li>
<li data-start="5977" data-end="6007">
<p data-start="5979" data-end="6007">Critical for chat interfaces</p>
</li>
</ul>
<h4 data-start="6009" data-end="6042">Time Per Output Token (TPOT)</h4>
<ul data-start="6043" data-end="6134">
<li data-start="6043" data-end="6092">
<p data-start="6045" data-end="6092">Speed of token generation after the first token</p>
</li>
<li data-start="6093" data-end="6134">
<p data-start="6095" data-end="6134">Determines how fast long responses feel</p>
</li>
</ul>
<p data-start="6136" data-end="6238">Two systems with the same total latency can feel very different depending on TTFT and TPOT trade-offs.</p>
<hr data-start="6240" data-end="6243">
<h3 data-start="6245" data-end="6274">Percentiles, Not Averages</h3>
<p data-start="6276" data-end="6302">Latency is a distribution.</p>
<p data-start="6304" data-end="6373">A single slow request can ruin the average. That’s why teams look at:</p>
<ul data-start="6374" data-end="6406">
<li data-start="6374" data-end="6388">
<p data-start="6376" data-end="6388">p50 (median)</p>
</li>
<li data-start="6389" data-end="6394">
<p data-start="6391" data-end="6394">p90</p>
</li>
<li data-start="6395" data-end="6400">
<p data-start="6397" data-end="6400">p95</p>
</li>
<li data-start="6401" data-end="6406">
<p data-start="6403" data-end="6406">p99</p>
</li>
</ul>
<p data-start="6408" data-end="6430">Outliers often signal:</p>
<ul data-start="6431" data-end="6528">
<li data-start="6431" data-end="6447">
<p data-start="6433" data-end="6447">Network issues</p>
</li>
<li data-start="6448" data-end="6467">
<p data-start="6450" data-end="6467">Oversized prompts</p>
</li>
<li data-start="6468" data-end="6528">
<p data-start="6470" data-end="6528">Resource contention <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
</li>
</ul>
<hr data-start="6530" data-end="6533">
<h3 data-start="6535" data-end="6558">Throughput and Cost</h3>
<p data-start="6560" data-end="6610">Throughput measures how much work the system does:</p>
<ul data-start="6611" data-end="6664">
<li data-start="6611" data-end="6636">
<p data-start="6613" data-end="6636">Tokens per second (TPS)</p>
</li>
<li data-start="6637" data-end="6664">
<p data-start="6639" data-end="6664">Requests per minute (RPM)</p>
</li>
</ul>
<p data-start="6666" data-end="6769">Higher throughput usually means lower cost—but pushing throughput too hard can destroy user experience.</p>
<hr data-start="6771" data-end="6774">
<h3 data-start="6776" data-end="6820">Goodput: Throughput That Actually Counts</h3>
<p data-start="6822" data-end="6903"><strong data-start="6822" data-end="6833">Goodput</strong> measures how many requests meet your service-level objectives (SLOs).</p>
<p data-start="6905" data-end="7016">If your system completes 100 requests/minute but only 30 meet latency targets, your goodput is <strong data-start="7000" data-end="7006">30</strong>, not 100.</p>
<p data-start="7018" data-end="7077">This metric prevents teams from optimizing the wrong thing.</p>
<hr data-start="7079" data-end="7082">
<h2 data-start="7084" data-end="7142">5. Hardware: Why GPUs, Memory, and Power Dominate Costs</h2>
<h3 data-start="7144" data-end="7174">What Is an AI Accelerator?</h3>
<p data-start="7176" data-end="7247">An accelerator is specialized hardware designed for specific workloads.</p>
<p data-start="7249" data-end="7407">For AI, the dominant accelerator is the <strong data-start="7289" data-end="7296">GPU</strong>, designed for massive parallelism—perfect for matrix multiplication, which dominates neural network workloads.</p>
<hr data-start="7409" data-end="7412">
<h3 data-start="7414" data-end="7443">Why GPUs Beat CPUs for AI</h3>
<ul data-start="7445" data-end="7556">
<li data-start="7445" data-end="7498">
<p data-start="7447" data-end="7498">CPUs: few powerful cores, good for sequential logic</p>
</li>
<li data-start="7499" data-end="7556">
<p data-start="7501" data-end="7556">GPUs: thousands of small cores, great for parallel math</p>
</li>
</ul>
<p data-start="7558" data-end="7697">More than 90% of neural network computation boils down to matrix multiplication, which GPUs excel at <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="7699" data-end="7702">
<h3 data-start="7704" data-end="7752">Memory Hierarchy Matters More Than You Think</h3>
<p data-start="7754" data-end="7801">Modern accelerators use multiple memory layers:</p>
<ul data-start="7802" data-end="7890">
<li data-start="7802" data-end="7826">
<p data-start="7804" data-end="7826">CPU DRAM (slow, large)</p>
</li>
<li data-start="7827" data-end="7852">
<p data-start="7829" data-end="7852">GPU HBM (fast, smaller)</p>
</li>
<li data-start="7853" data-end="7890">
<p data-start="7855" data-end="7890">On-chip SRAM (extremely fast, tiny)</p>
</li>
</ul>
<p data-start="7892" data-end="7985">Inference optimization is often about <strong data-start="7930" data-end="7962">moving data less and smarter</strong> across this hierarchy.</p>
<hr data-start="7987" data-end="7990">
<h3 data-start="7992" data-end="8024">Power Is a Hidden Bottleneck</h3>
<p data-start="8026" data-end="8064">High-end GPUs consume enormous energy:</p>
<ul data-start="8065" data-end="8172">
<li data-start="8065" data-end="8119">
<p data-start="8067" data-end="8119">An H100 running continuously can use ~7,000 kWh/year</p>
</li>
<li data-start="8120" data-end="8172">
<p data-start="8122" data-end="8172">Comparable to a household’s annual electricity use</p>
</li>
</ul>
<p data-start="8174" data-end="8273">This makes power—and cooling—a real constraint on AI scaling <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="8275" data-end="8278">
<h2 data-start="8280" data-end="8344">6. Model-Level Optimization: Making Models Lighter and Faster</h2>
<h3 data-start="8346" data-end="8378">Model Compression Techniques</h3>
<p data-start="8380" data-end="8417">Several techniques reduce model size:</p>
<h4 data-start="8419" data-end="8436">Quantization</h4>
<ul data-start="8437" data-end="8558">
<li data-start="8437" data-end="8493">
<p data-start="8439" data-end="8493">Reduce numerical precision (FP32 → FP16 → INT8 → INT4)</p>
</li>
<li data-start="8494" data-end="8510">
<p data-start="8496" data-end="8510">Smaller models</p>
</li>
<li data-start="8511" data-end="8529">
<p data-start="8513" data-end="8529">Faster inference</p>
</li>
<li data-start="8530" data-end="8558">
<p data-start="8532" data-end="8558">Lower memory bandwidth use</p>
</li>
</ul>
<h4 data-start="8560" data-end="8577">Distillation</h4>
<ul data-start="8578" data-end="8654">
<li data-start="8578" data-end="8623">
<p data-start="8580" data-end="8623">Train a smaller model to mimic a larger one</p>
</li>
<li data-start="8624" data-end="8654">
<p data-start="8626" data-end="8654">Often surprisingly effective</p>
</li>
</ul>
<h4 data-start="8656" data-end="8668">Pruning</h4>
<ul data-start="8669" data-end="8766">
<li data-start="8669" data-end="8700">
<p data-start="8671" data-end="8700">Remove unimportant parameters</p>
</li>
<li data-start="8701" data-end="8724">
<p data-start="8703" data-end="8724">Creates sparse models</p>
</li>
<li data-start="8725" data-end="8766">
<p data-start="8727" data-end="8766">Less common due to hardware limitations</p>
</li>
</ul>
<p data-start="8768" data-end="8884">Among these, <strong data-start="8781" data-end="8809">weight-only quantization</strong> is the most widely used in practice <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="8886" data-end="8889">
<h3 data-start="8891" data-end="8924">The Autoregressive Bottleneck</h3>
<p data-start="8926" data-end="8965">Generating text one token at a time is:</p>
<ul data-start="8966" data-end="9009">
<li data-start="8966" data-end="8972">
<p data-start="8968" data-end="8972">Slow</p>
</li>
<li data-start="8973" data-end="8984">
<p data-start="8975" data-end="8984">Expensive</p>
</li>
<li data-start="8985" data-end="9009">
<p data-start="8987" data-end="9009">Memory-bandwidth heavy</p>
</li>
</ul>
<p data-start="9011" data-end="9078">Several techniques attempt to overcome this fundamental limitation.</p>
<hr data-start="9080" data-end="9083">
<h3 data-start="9085" data-end="9109">Speculative Decoding</h3>
<p data-start="9111" data-end="9116">Idea:</p>
<ul data-start="9117" data-end="9215">
<li data-start="9117" data-end="9169">
<p data-start="9119" data-end="9169">Use a smaller “draft” model to guess future tokens</p>
</li>
<li data-start="9170" data-end="9215">
<p data-start="9172" data-end="9215">Have the main model verify them in parallel</p>
</li>
</ul>
<p data-start="9217" data-end="9351">If many draft tokens are accepted, the system generates multiple tokens per step—dramatically improving speed without hurting quality.</p>
<p data-start="9353" data-end="9423">This technique is now widely supported in modern inference frameworks.</p>
<hr data-start="9425" data-end="9428">
<h3 data-start="9430" data-end="9458">Inference with Reference</h3>
<p data-start="9460" data-end="9574">Instead of generating text the model already knows (e.g., copied context), simply <strong data-start="9542" data-end="9573">reuse tokens from the input</strong>.</p>
<p data-start="9576" data-end="9607">This works especially well for:</p>
<ul data-start="9608" data-end="9664">
<li data-start="9608" data-end="9622">
<p data-start="9610" data-end="9622">Document Q&amp;A</p>
</li>
<li data-start="9623" data-end="9637">
<p data-start="9625" data-end="9637">Code editing</p>
</li>
<li data-start="9638" data-end="9664">
<p data-start="9640" data-end="9664">Multi-turn conversations</p>
</li>
</ul>
<p data-start="9666" data-end="9723">It avoids redundant computation and speeds up generation.</p>
<hr data-start="9725" data-end="9728">
<h3 data-start="9730" data-end="9751">Parallel Decoding</h3>
<p data-start="9753" data-end="9823">Some techniques try to generate multiple future tokens simultaneously.</p>
<p data-start="9825" data-end="9834">Examples:</p>
<ul data-start="9835" data-end="9864">
<li data-start="9835" data-end="9855">
<p data-start="9837" data-end="9855">Lookahead decoding</p>
</li>
<li data-start="9856" data-end="9864">
<p data-start="9858" data-end="9864">Medusa</p>
</li>
</ul>
<p data-start="9866" data-end="9961">These approaches are promising but complex, requiring careful verification to ensure coherence.</p>
<hr data-start="9963" data-end="9966">
<h2 data-start="9968" data-end="10027">7. Attention Optimization: Taming the KV Cache Explosion</h2>
<h3 data-start="10029" data-end="10059">Why Attention Is Expensive</h3>
<p data-start="10061" data-end="10107">Each new token attends to all previous tokens.</p>
<p data-start="10109" data-end="10130">Without optimization:</p>
<ul data-start="10131" data-end="10205">
<li data-start="10131" data-end="10164">
<p data-start="10133" data-end="10164">Computation grows quadratically</p>
</li>
<li data-start="10165" data-end="10205">
<p data-start="10167" data-end="10205">KV cache grows linearly—but still huge</p>
</li>
</ul>
<p data-start="10207" data-end="10325">For large models and long contexts, the KV cache can exceed model size itself <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="10327" data-end="10330">
<h3 data-start="10332" data-end="10368">KV Cache Optimization Techniques</h3>
<p data-start="10370" data-end="10399">Three broad strategies exist:</p>
<h4 data-start="10401" data-end="10427">Redesigning Attention</h4>
<ul data-start="10428" data-end="10526">
<li data-start="10428" data-end="10452">
<p data-start="10430" data-end="10452">Local window attention</p>
</li>
<li data-start="10453" data-end="10476">
<p data-start="10455" data-end="10476">Multi-query attention</p>
</li>
<li data-start="10477" data-end="10502">
<p data-start="10479" data-end="10502">Grouped-query attention</p>
</li>
<li data-start="10503" data-end="10526">
<p data-start="10505" data-end="10526">Cross-layer attention</p>
</li>
</ul>
<p data-start="10528" data-end="10581">These reduce how much data must be stored and reused.</p>
<hr data-start="10583" data-end="10586">
<h4 data-start="10588" data-end="10620">Optimizing KV Cache Storage</h4>
<p data-start="10622" data-end="10654">Frameworks like vLLM introduced:</p>
<ul data-start="10655" data-end="10724">
<li data-start="10655" data-end="10671">
<p data-start="10657" data-end="10671">PagedAttention</p>
</li>
<li data-start="10672" data-end="10700">
<p data-start="10674" data-end="10700">Flexible memory allocation</p>
</li>
<li data-start="10701" data-end="10724">
<p data-start="10703" data-end="10724">Reduced fragmentation</p>
</li>
</ul>
<p data-start="10726" data-end="10743">Other approaches:</p>
<ul data-start="10744" data-end="10810">
<li data-start="10744" data-end="10767">
<p data-start="10746" data-end="10767">KV cache quantization</p>
</li>
<li data-start="10768" data-end="10790">
<p data-start="10770" data-end="10790">Adaptive compression</p>
</li>
<li data-start="10791" data-end="10810">
<p data-start="10793" data-end="10810">Selective caching</p>
</li>
</ul>
<hr data-start="10812" data-end="10815">
<h4 data-start="10817" data-end="10844">Writing Better Kernels</h4>
<p data-start="10846" data-end="10929">Instead of changing algorithms, optimize how computations are executed on hardware.</p>
<p data-start="10931" data-end="10955">The most famous example:</p>
<ul data-start="10956" data-end="11058">
<li data-start="10956" data-end="10976">
<p data-start="10958" data-end="10976"><strong data-start="10958" data-end="10976">FlashAttention</strong></p>
</li>
<li data-start="10977" data-end="11004">
<p data-start="10979" data-end="11004">Fuses multiple operations</p>
</li>
<li data-start="11005" data-end="11030">
<p data-start="11007" data-end="11030">Minimizes memory access</p>
</li>
<li data-start="11031" data-end="11058">
<p data-start="11033" data-end="11058">Huge speedups in practice</p>
</li>
</ul>
<hr data-start="11060" data-end="11063">
<h2 data-start="11065" data-end="11127">8. Service-Level Optimization: Making the Whole System Work</h2>
<h3 data-start="11129" data-end="11166">Batching: The Simplest Cost Saver</h3>
<p data-start="11168" data-end="11204">Batching combines multiple requests:</p>
<ul data-start="11205" data-end="11261">
<li data-start="11205" data-end="11226">
<p data-start="11207" data-end="11226">Improves throughput</p>
</li>
<li data-start="11227" data-end="11241">
<p data-start="11229" data-end="11241">Reduces cost</p>
</li>
<li data-start="11242" data-end="11261">
<p data-start="11244" data-end="11261">Increases latency</p>
</li>
</ul>
<p data-start="11263" data-end="11269">Types:</p>
<ul data-start="11270" data-end="11328">
<li data-start="11270" data-end="11287">
<p data-start="11272" data-end="11287">Static batching</p>
</li>
<li data-start="11288" data-end="11306">
<p data-start="11290" data-end="11306">Dynamic batching</p>
</li>
<li data-start="11307" data-end="11328">
<p data-start="11309" data-end="11328">Continuous batching</p>
</li>
</ul>
<p data-start="11330" data-end="11389">The trick is batching <strong data-start="11352" data-end="11388">without hurting latency too much</strong>.</p>
<hr data-start="11391" data-end="11394">
<h3 data-start="11396" data-end="11421">Compilers and Kernels</h3>
<p data-start="11423" data-end="11458">Modern inference relies heavily on:</p>
<ul data-start="11459" data-end="11529">
<li data-start="11459" data-end="11501">
<p data-start="11461" data-end="11501">Compilers (torch.compile, XLA, TensorRT)</p>
</li>
<li data-start="11502" data-end="11529">
<p data-start="11504" data-end="11529">Hardware-specific kernels</p>
</li>
</ul>
<p data-start="11531" data-end="11612">These translate high-level model code into highly optimized machine instructions.</p>
<p data-start="11614" data-end="11759">Many companies treat their kernels as trade secrets because they directly translate into cost advantages <span class="" data-state="closed"><span class="relative inline-flex items-center"><button class="ms-1 flex h-[25px] text-[10px] leading-[13px] rounded-xl corner-superellipse/1.1 items-center justify-center gap-1 px-2 relative text-token-text-secondary! hover:text-token-text-primary! hover:bg-token-bg-secondary dark:bg-token-main-surface-secondary dark:hover:bg-token-bg-secondary bg-[#f4f4f4] "><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" aria-hidden="true" class="h-[16px] w-[16px] object-contain text-token-text-primary! flex-none icon-sm"><use href="/cdn/assets/sprites-core-i9agxugi.svg#554074" fill="currentColor"></use></svg><p class="not-prose mt-0! mb-0! flex-auto truncate">ch9</p></button></span></span>.</p>
<hr data-start="11761" data-end="11764">
<h2 data-start="11766" data-end="11832">Conclusion: Inference Optimization Is the Real Production Skill</h2>
<p data-start="11834" data-end="11912">Training models gets headlines. Inference optimization keeps businesses alive.</p>
<p data-start="11914" data-end="11937">Inference optimization:</p>
<ul data-start="11938" data-end="12077">
<li data-start="11938" data-end="11966">
<p data-start="11940" data-end="11966">Determines user experience</p>
</li>
<li data-start="11967" data-end="11993">
<p data-start="11969" data-end="11993">Dominates long-term cost</p>
</li>
<li data-start="11994" data-end="12032">
<p data-start="11996" data-end="12032">Requires cross-disciplinary thinking</p>
</li>
<li data-start="12033" data-end="12077">
<p data-start="12035" data-end="12077">Is where real-world AI engineering happens</p>
</li>
</ul>
<p data-start="12079" data-end="12161">As models become commoditized, <strong data-start="12110" data-end="12160">efficient inference becomes a competitive moat</strong>.</p>
<p data-start="12163" data-end="12296">The future of AI won’t be decided by who trains the biggest model—but by who can run models <strong data-start="12255" data-end="12295">fastest, cheapest, and most reliably</strong>.</p>
<hr data-start="12298" data-end="12301">


</div>

<h3>Addendum</h3>

<b>My prompt:</b>

<pre><i>Role: You an expert in AI Engineering and a prolific writer. 
Task: Spin this attached chapter 9 as a blog post in layman terms 
Rules: Organize the post in 7-8 sections with subsections as needed 
Note: Blog post should be about 6000 to 7000 words long
Note: Try not to miss any important section or details</i></pre>

<span style="opacity: 0;">Tags: Artificial Intelligence,Generative AI,Agentic AI,Technology,Book Summary,</span>
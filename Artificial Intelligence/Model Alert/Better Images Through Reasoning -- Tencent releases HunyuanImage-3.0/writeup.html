<head>
    <script crossorigin="anonymous" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
    
    <!-- Google AdSense Using Machine Learning Code -->
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-3071098372371409",
            enable_page_level_ads: true
        });
    </script>
    
    <script>
        $(document).ready(function () {
            $.ajax({
                url: "https://raw.githubusercontent.com/ashishjain1547/pubLessonsInTechnology/main/links_to_tech_clubs.json",
                success: function (result) {
                    let grouplink = JSON.parse(result)['Beta Tech Club'];
                    $("#customWhatsAppGroupLinkWrapper").html(
                        `
                        <h2 class="custom_link_h2"><a href="${grouplink}" target="_blank"> 
                            <span>Join us on:</span>
                            <span class="customLink"><i class="fa fa-whatsapp"></i> Whatsapp </span>
                            </a>
                        </h2>
                        `
                    );
                }
            });
        });
    </script>
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    
    <style>
        pre {
            white-space: pre-wrap;
            white-space: -moz-pre-wrap;
            white-space: -pre-wrap;
            white-space: -o-pre-wrap;
            word-wrap: break-word;
        }
    
        .customLink {
            background-color: #4CAF50;
            border: none;
            color: white !important;
            padding: 8px 13px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 14px;
            margin: 4px 2px;
            cursor: pointer;
        }
    
        .customLink:hover {
            text-decoration: none;
        }
    
        div.code-block-decoration.footer {
            display: none;
        }
    
        button.export-sheets-button-wrapper {
            display: none;
        }
    </style>
    
    <style>
        .custom_link_h2 a {
            color: black;
            text-decoration: none;
            text-align: center;
        }
    
        .custom_link_h2 a:hover {
            color: black;
        }
    
        .custom_link_h2 a:active {
            color: black;
        }
    
        .custom_link_h2 span {
            translate: 0px -5px;
            display: inline-block;
        }
    
        .custom_link_h2 img {
            width: 100px;
            padding: 0px;
            border: none;
            box-shadow: none;
        }
    </style>
    <style>
        .customul {
            list-style: none;
        }
    
        [aria-hidden='true'] {
            display: none;
        }
    
        .custom_iframe {
            width: 100%;
            height: 305px;
        }
    
        i.ir { color: red; }
        i.ig { color: green; }
        i.ib { color: blue; }
        i.im { color: magenta; }
        i.ip { color: purple; }
    
        .customTable td {
            padding: 2px;
        }
    
        i.green {
            color: green;
        }
    
        i.red {
            color: red;
        }
    
        i.blue {
            color: blue;
        }
    
        button.flex.gap-1.items-center.select-none.px-4.py-1 {
            display: none;
        }
    
        button.flex.select-none.items-center.gap-1 {
            display: none;
        }

        button.bg-token-bg-primary {
            display: none;
        }
    
        .flex.items-center {
            display: none;
        }
    </style>
</head>

<div id="customWhatsAppGroupLinkWrapper"></div>
<br />
<a class="customLink" href="https://survival8.blogspot.com/p/index-of-lessons-in-technology.html#customArtificialIntelligence" target="_blank">See All Articles on AI</a>
<br>
<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqwPVcbh9dn9mOVkYugv2wXMxkhWZH1O0xOnT0Q7Lm5R4E_3qOK1k4id5QBauRraLQQyxegDuXdPHLtVBthsJMek_c5D6kriGlFfKIiDlczgo5EeWO5bans5dYCpd2PWWD3qDjxD0mo9dSY8oZekjiQfasI3ZHKVtmU7vomyeCFh4NaZa9SxtQaDqnMd0E/s1200/unnamed.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" width="600" data-original-height="675" data-original-width="1200" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqwPVcbh9dn9mOVkYugv2wXMxkhWZH1O0xOnT0Q7Lm5R4E_3qOK1k4id5QBauRraLQQyxegDuXdPHLtVBthsJMek_c5D6kriGlFfKIiDlczgo5EeWO5bans5dYCpd2PWWD3qDjxD0mo9dSY8oZekjiQfasI3ZHKVtmU7vomyeCFh4NaZa9SxtQaDqnMd0E/s600/unnamed.png"/></a></div>
<br>
<div id="m_-7895511608593139206hs_cos_wrapper_hs_email_body_old9_" style="color:inherit;font-size:inherit;line-height:inherit">
<p style="line-height:150%">&nbsp;</p>
<p style="line-height:150%">A new image generator reasons over&nbsp;prompts to produce outstanding pictures.</p>
<p style="line-height:150%">&nbsp;</p>
<p style="line-height:150%"><span style="font-weight:bold">What’s new:</span> Tencent released <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCd_W3qgz0W7lCdLW6lZ3ltW72RXkl2WxRQ7W3cppv02MwG1NW47g7_r5L6RPsN3kYJ7DYdMP5N8KPHG_X589VW73k0nV9gjyPRW1dNc534CSj61W98pb4r5Lgrj3W8pdS342zzPbTW3glqBw8Zd8pvW5q_WWD4D13SGN8lkQcsCmdG1W1P_Bwz7Vv9gCW48bf519lL8pqW49JCyT2sjDj2Vj6sj98bSCfCW4mPkdS8sx5WtN55-RRpvvXdZW2VbhG52TfK05W2CSjZB7kf9syW2FxxNl4Y81c9N1LXQlDM21FMW2mkGbT8V8pLfVmzYgf9m05j-f1KtRNl04" rel="noopener" style="color:#237b94" target="_blank" data-saferedirecturl="https://www.google.com/url?q=https://info.deeplearning.ai/e3t/Ctc/LX%2B113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCd_W3qgz0W7lCdLW6lZ3ltW72RXkl2WxRQ7W3cppv02MwG1NW47g7_r5L6RPsN3kYJ7DYdMP5N8KPHG_X589VW73k0nV9gjyPRW1dNc534CSj61W98pb4r5Lgrj3W8pdS342zzPbTW3glqBw8Zd8pvW5q_WWD4D13SGN8lkQcsCmdG1W1P_Bwz7Vv9gCW48bf519lL8pqW49JCyT2sjDj2Vj6sj98bSCfCW4mPkdS8sx5WtN55-RRpvvXdZW2VbhG52TfK05W2CSjZB7kf9syW2FxxNl4Y81c9N1LXQlDM21FMW2mkGbT8V8pLfVmzYgf9m05j-f1KtRNl04&amp;source=gmail&amp;ust=1763252898376000&amp;usg=AOvVaw0oGyhdqqKF1uPQmOBeGE90">HunyuanImage-3.0</a>, which is fine-tuned to apply reasoning via a variety of&nbsp;reinforcement learning methods. The company says this helps it understand users’ intentions and improve its output.</p>
<ul style="line-height:150%">
<li aria-level="1"><strong>Input/output:</strong> Text and images in, text and images out (fine-tuned for text in, images out only)&nbsp;</li>
<li aria-level="1"><strong>Architecture:</strong> Mixture of experts (MoE) diffusion transformer (80 billion parameters, 13 billion parameters active per token), one VAE, one vision transformer, two vanilla neural network projectors</li>
<li aria-level="1"><strong>Performance:</strong> Currently tops LMArena Text-to-Image leaderboard</li>
<li aria-level="1"><span style="font-weight:bold">Availability:</span> Weights <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0b3qgz0W7Y8-PT6lZ3nMW7dvJ0Z1hRL_3W8qFR4p7mbKNPW33Gwmc7s59nHW8TBDDx6sKBfWW20t2KM5DHGlrW5LgHby4X1MBfW8dBw3X8KT3LQN81_LN8m--BLW8TySDR9fGBrXN2S8sBh--xLFW9flMDB1ttGm-W76Xl7Y6dmN_YW32Q9_-2gVQffW1Swsvy3B159MVtzSyt6dcLvdW4Ltvfk5Z_8mKW6_pHDc5lfgr4W7RBBVH6NfgCLW4p6Krw8BY291W1w9sPb92b7_5W42D3bF4Mm8X9W6Yp0Gt8Tl6JVW82g5JF650JDKW7BcnFW7x07rhW1rzygn96L15lW6ksLVv6SsWTfd4Cb8M04" rel="noopener" style="color:#237b94" target="_blank" data-saferedirecturl="https://www.google.com/url?q=https://info.deeplearning.ai/e3t/Ctc/LX%2B113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0b3qgz0W7Y8-PT6lZ3nMW7dvJ0Z1hRL_3W8qFR4p7mbKNPW33Gwmc7s59nHW8TBDDx6sKBfWW20t2KM5DHGlrW5LgHby4X1MBfW8dBw3X8KT3LQN81_LN8m--BLW8TySDR9fGBrXN2S8sBh--xLFW9flMDB1ttGm-W76Xl7Y6dmN_YW32Q9_-2gVQffW1Swsvy3B159MVtzSyt6dcLvdW4Ltvfk5Z_8mKW6_pHDc5lfgr4W7RBBVH6NfgCLW4p6Krw8BY291W1w9sPb92b7_5W42D3bF4Mm8X9W6Yp0Gt8Tl6JVW82g5JF650JDKW7BcnFW7x07rhW1rzygn96L15lW6ksLVv6SsWTfd4Cb8M04&amp;source=gmail&amp;ust=1763252898376000&amp;usg=AOvVaw1B8hrtYyVmL45EIz5inyur">available</a> for commercial and noncommercial use by companies with fewer&nbsp;than 100 million monthly active users under Tencent <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0v3qgz0W8wLKSR6lZ3lPW7rgWy72sflv2W4H1Vz27PVPr0T2M2F9l-dMWW6gBF9Q5PqVMtW8kqHbM77fjwXW64-7Cf1sNgN1W2f9XCg5mhGFCW1gslv18pswH8Vwb_Ld6RqHh7W2tLqTr7-XWNRW26LZQT6m2d-XW2BPZVb1qG2vhW2l1qM35Vn-66W4_05Xy515D-XW8VlllT7FJGlLW498prC3NH8l0W49f6hV8DHlPtW1Yq6_t8Hcc6TW7N0C7M2yrDptW6ZymCB5-0THvW6jD9k53y6zD_W3vsLYw55LvzDW7y8hw_6g9RXBVypfQv6N6qJhW9gs2F02TyVtlVX79XW66qqftW5KBWt35bsD8hM4hRFlwfT03f3FJkq004" rel="noopener" style="color:#237b94" target="_blank" data-saferedirecturl="https://www.google.com/url?q=https://info.deeplearning.ai/e3t/Ctc/LX%2B113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0v3qgz0W8wLKSR6lZ3lPW7rgWy72sflv2W4H1Vz27PVPr0T2M2F9l-dMWW6gBF9Q5PqVMtW8kqHbM77fjwXW64-7Cf1sNgN1W2f9XCg5mhGFCW1gslv18pswH8Vwb_Ld6RqHh7W2tLqTr7-XWNRW26LZQT6m2d-XW2BPZVb1qG2vhW2l1qM35Vn-66W4_05Xy515D-XW8VlllT7FJGlLW498prC3NH8l0W49f6hV8DHlPtW1Yq6_t8Hcc6TW7N0C7M2yrDptW6ZymCB5-0THvW6jD9k53y6zD_W3vsLYw55LvzDW7y8hw_6g9RXBVypfQv6N6qJhW9gs2F02TyVtlVX79XW66qqftW5KBWt35bsD8hM4hRFlwfT03f3FJkq004&amp;source=gmail&amp;ust=1763252898376000&amp;usg=AOvVaw2qnuXPGd5VUL6kR7NJDsQW">license</a></li>
<li aria-level="1"><strong>Undisclosed:</strong> Input and output size limits; parameter counts of VAE, vision transformer, and projectors; training data; models used for labeling, filtering, and captioning images; reward models</li>
</ul>
<p style="line-height:150%"><span style="font-weight:bold">How it works:</span> The authors built a training dataset of paired text and images. They trained the model on image generation via diffusion through several stages and fine-tuned it on text-to-image generation in&nbsp;further stages.</p>
<ul style="line-height:150%">
<li aria-level="1">To produce the dataset, the authors collected 10 billion images. (i) They built models specially trained to measure image clarity and aesthetic quality, and removed images that didn’t make the grade. (ii) They also built models to identify text and named entities such as brands, artworks, and celebrities, and extracted this information from the remaining images. (iii) They fed the images, extracted text, and extracted entities to a captioning model that&nbsp;produced a text caption for each image. (iv) For a subset of the data, they manually annotated chains of thought, producing data that linked text to chains of thought to images. (v) They added text-to-text data and image-text data from unspecified corpi.</li>
<li aria-level="1">The authors pretrained the system to generate text and images from the various text and image elements in the dataset. Specifically, for text-to-image tasks: (i) First, the VAE’s encoder embedded an image. (ii) The authors added noise to the embedding. (iii) Given the noisy embedding and a text prompt, the MoE removed the noise. (iv) The VAE’s decoder generated an image from the embedding with noise removed.</li>
<li aria-level="1">The authors fine-tuned the system (i) for text-to-image tasks by training it in a supervised fashion to remove noise from human-annotated examples, (ii) via <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0P3qgz0W95jsWP6lZ3pDW8x7Q_58Cg0HyN6CD9SMh-34HW7gjyS012_X0yW5dRL0v7LLF97W2SMQCX3jbJYqW1Rdlhb6VMNSQW8JLkwW2nKPZbW5Qh92H62G25bW7V-P_h3c4h5PVNXN9m3lQ7QwW89N4JM2vhQSZW4G3GDQ5147X3W51rSsP6CYZNJW7szrq_4pN-C6W7-d8K33yN9JdVMtKQN2B9qRqW449H3q1hwJbRW1n32qR3_JhLBN2jqt5SfkKtMW1TDQr9645vX6W88NZ7417KpdJW3JKRmn4yKMbYW5zJ7dK9j-8SPW4QzV-l25mFmgW4gRR7R24C-p1W27M7Df4ZCtmdW3xSYH94Kb-dCW69nWQz3QbZCbW7V7md237fKqhW3t8sY786D54kf2tRs0W04" rel="noopener" style="color:#237b94" target="_blank" data-saferedirecturl="https://www.google.com/url?q=https://info.deeplearning.ai/e3t/Ctc/LX%2B113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0P3qgz0W95jsWP6lZ3pDW8x7Q_58Cg0HyN6CD9SMh-34HW7gjyS012_X0yW5dRL0v7LLF97W2SMQCX3jbJYqW1Rdlhb6VMNSQW8JLkwW2nKPZbW5Qh92H62G25bW7V-P_h3c4h5PVNXN9m3lQ7QwW89N4JM2vhQSZW4G3GDQ5147X3W51rSsP6CYZNJW7szrq_4pN-C6W7-d8K33yN9JdVMtKQN2B9qRqW449H3q1hwJbRW1n32qR3_JhLBN2jqt5SfkKtMW1TDQr9645vX6W88NZ7417KpdJW3JKRmn4yKMbYW5zJ7dK9j-8SPW4QzV-l25mFmgW4gRR7R24C-p1W27M7Df4ZCtmdW3xSYH94Kb-dCW69nWQz3QbZCbW7V7md237fKqhW3t8sY786D54kf2tRs0W04&amp;source=gmail&amp;ust=1763252898376000&amp;usg=AOvVaw0eDJcgm--stKHbyklM82JL">DPO</a>&nbsp;to be more likely to generate higher-quality examples, like human-annotated ones,&nbsp;than lower-quality ones, (iii) via the reinforcement learning method <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCd_W3qgz0W7lCdLW6lZ3pmV12Q5f2m8D_GW6vZcVZ2cnW13W4x8gKw2YMH5lW5qf-GN9gzyM3W7VzZsC2YlYX1W6bzd_X8qWYH_W4MSfK-3h67qpW7Xh_SZ6mwtVsW6DqvY51gXWrCW3BwYd731n40yW4W-wNQ2Vf8rdN3Q2p3g3Pk7mW4dYjRh4ymP2HW64y20X6FSYQvVzhJMs77-b4zW2CM0vd8s2sKYW5Kjj9678bNSsW7n2W0D2HqffqW4-QDbp7PwLlfW3z8Px_3rLcgFW7KvRFr2sJV08W2tPTrM3X-X72W8rXWYM6GSsPdMxRqW0yY7lMf3qnDT804" rel="noopener" style="color:#237b94" target="_blank" data-saferedirecturl="https://www.google.com/url?q=https://info.deeplearning.ai/e3t/Ctc/LX%2B113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCd_W3qgz0W7lCdLW6lZ3pmV12Q5f2m8D_GW6vZcVZ2cnW13W4x8gKw2YMH5lW5qf-GN9gzyM3W7VzZsC2YlYX1W6bzd_X8qWYH_W4MSfK-3h67qpW7Xh_SZ6mwtVsW6DqvY51gXWrCW3BwYd731n40yW4W-wNQ2Vf8rdN3Q2p3g3Pk7mW4dYjRh4ymP2HW64y20X6FSYQvVzhJMs77-b4zW2CM0vd8s2sKYW5Kjj9678bNSsW7n2W0D2HqffqW4-QDbp7PwLlfW3z8Px_3rLcgFW7KvRFr2sJV08W2tPTrM3X-X72W8rXWYM6GSsPdMxRqW0yY7lMf3qnDT804&amp;source=gmail&amp;ust=1763252898376000&amp;usg=AOvVaw2Cdtcl3a1jekqfAo6p5Y6D">MixGRPO</a>&nbsp;to encourage the model to generate more aesthetically pleasing images as judged by unspecified reward models, and (iv) via <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCd_W3qgz0W7lCdLW6lZ3l6W5cFmm68VVwyZW1ggcwn86v3wFW7Ghwsw1FT8crW3FNLnB1zmCSbW4SxYtV7NvvzDW9lTzdL8Y4hMMW4jPdcW1d2WJFW5w5Cm15xRdmSVZZ6SN5PSpzTW2VGj652mbrZYN97nPLVtJ5P7W1SQxDL51J5dXW7drmJ411S4dVN1cn5PkHDt8LW3PHFNL8kZT-nW5-8X3s1dCDKMW1j5t322PVqZjN4tjb6CQcGVtW9c2LM91pvCQQW7nCbjD3rPCx3W93cd-33_XkgKW5ZxV0-1sXjj7VL1v7H5kwHZrW1ld28c78RXGTf3qbSxn04" rel="noopener" style="color:#237b94" target="_blank" data-saferedirecturl="https://www.google.com/url?q=https://info.deeplearning.ai/e3t/Ctc/LX%2B113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCd_W3qgz0W7lCdLW6lZ3l6W5cFmm68VVwyZW1ggcwn86v3wFW7Ghwsw1FT8crW3FNLnB1zmCSbW4SxYtV7NvvzDW9lTzdL8Y4hMMW4jPdcW1d2WJFW5w5Cm15xRdmSVZZ6SN5PSpzTW2VGj652mbrZYN97nPLVtJ5P7W1SQxDL51J5dXW7drmJ411S4dVN1cn5PkHDt8LW3PHFNL8kZT-nW5-8X3s1dCDKMW1j5t322PVqZjN4tjb6CQcGVtW9c2LM91pvCQQW7nCbjD3rPCx3W93cd-33_XkgKW5ZxV0-1sXjj7VL1v7H5kwHZrW1ld28c78RXGTf3qbSxn04&amp;source=gmail&amp;ust=1763252898376000&amp;usg=AOvVaw0S4MJAglYPHjCel6tgGzVl">SRPO</a> (another reinforcement learning method) to encourage the model to generate images more like a text description that specified desired traits and less like a text description that specified negative traits. While applying SRPO, they also encouraged the model to generate images similar to those in an author-chosen distribution.</li>
</ul>
<p style="line-height:150%"><span style="font-weight:bold">Results:</span> At present, HunyuanImage 3.0 holds first place in the LMArena Text-to-Image leaderboard, ahead of Google Gemini 2.5 Flash Image (Nano Banana), Google Imagen 4.0 Ultra Generate, and ByteDance Seedream 4.0. In addition, 100 people compared 1,000 outputs of 4 competing models to those of HunyuanImage 3.0 in side-by-side contests. The people evaluated which image was better, or whether they were both equally good or equally poor.</p>
<ul style="line-height:150%">
<li aria-level="1">On average, the people preferred HunyuanImage 3.0’s images over those of the competitors.&nbsp;</li>
<li aria-level="1">For example, 20.01 percent of the time they preferred HunyuanImage 3.0, 18.84 percent of the time they preferred Seedream 4.0, 39.3 percent of the time they were equally good, and 21.85 percent of the time they were equally poor.</li>
</ul>
<p style="line-height:150%"><span style="font-weight:bold">Behind the news:</span> Tencent has been on a streak of releasing vision models.&nbsp;</p>
<ul style="line-height:150%">
<li aria-level="1">Tencent recently launched the API version of <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0b3qgz0W7Y8-PT6lZ3ldW4L-Nkv3jz8kpW7HKcW953c7TRW45fBM18y_FcMW6-cQxY3sNJnPW6zF7yL13CJSRW21wV9t86r5LyW9lvmLW85fLfGN7d2QbG8mJ58W75QsP043JbpWW2Lr8RG4JkHhTW1Q6Z0T7FXBtZW7C3cD83dDBv9W4V7mzV794nSjN2-4tSPRfT6fV2k8MK8hCGxzW2wyw326j6tVSW2f5vr64BRz_QW7hGJL47_VGHCW8bXD9w2JbRpCW2Cgb5W6sjkWdW25K-8q4v37jyW8fy_m-48x5JkW3C1sWX1C6hv-W60n5s43X1YZ-W3dgFX25VnWBFW23MrJM8Jjv2Yf2nJxMM04" rel="noopener" style="color:#237b94" target="_blank" data-saferedirecturl="https://www.google.com/url?q=https://info.deeplearning.ai/e3t/Ctc/LX%2B113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0b3qgz0W7Y8-PT6lZ3ldW4L-Nkv3jz8kpW7HKcW953c7TRW45fBM18y_FcMW6-cQxY3sNJnPW6zF7yL13CJSRW21wV9t86r5LyW9lvmLW85fLfGN7d2QbG8mJ58W75QsP043JbpWW2Lr8RG4JkHhTW1Q6Z0T7FXBtZW7C3cD83dDBv9W4V7mzV794nSjN2-4tSPRfT6fV2k8MK8hCGxzW2wyw326j6tVSW2f5vr64BRz_QW7hGJL47_VGHCW8bXD9w2JbRpCW2Cgb5W6sjkWdW25K-8q4v37jyW8fy_m-48x5JkW3C1sWX1C6hv-W60n5s43X1YZ-W3dgFX25VnWBFW23MrJM8Jjv2Yf2nJxMM04&amp;source=gmail&amp;ust=1763252898376000&amp;usg=AOvVaw3ABqWqj7njgcuOLhiR2uYQ">Hunyuan-Vision-1.5</a>, its latest vision-language model, with promises to release the weights and a paper soon.</li>
<li aria-level="1">The company released <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0b3qgz0W7Y8-PT6lZ3pcW4Wrc927dCq63W2H25_227tqxRW7My43g7D3_q3W1sMYMQ7jW5T7W584HcZ3JWxM6W1G9pB_62bbb0W52KkQM5bttfKVXFfpB2qKGYxW5D2KsN2HBVdgW3KNHPC1-t9skN6DgZ8LCcr02W89YRXH25cmC1W983qvg81mbR2N6CfB9ZG3CpYW5pl5kD7XX5dTW78M3Hm85BlmsW9lfr0Z8k7JDvW16Dy7y4yPc6QM1XQZlPXyYrW1tPpm-24_zdtW227x9t48jlRqW8H5lz48mFDmgW6fXtM_2VsjGDV9lk5K3CFCNHW3dv9Np7VzTjLW2HQJv21LqFftf4Xkrlj04" rel="noopener" style="color:#237b94" target="_blank" data-saferedirecturl="https://www.google.com/url?q=https://info.deeplearning.ai/e3t/Ctc/LX%2B113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCf0b3qgz0W7Y8-PT6lZ3pcW4Wrc927dCq63W2H25_227tqxRW7My43g7D3_q3W1sMYMQ7jW5T7W584HcZ3JWxM6W1G9pB_62bbb0W52KkQM5bttfKVXFfpB2qKGYxW5D2KsN2HBVdgW3KNHPC1-t9skN6DgZ8LCcr02W89YRXH25cmC1W983qvg81mbR2N6CfB9ZG3CpYW5pl5kD7XX5dTW78M3Hm85BlmsW9lfr0Z8k7JDvW16Dy7y4yPc6QM1XQZlPXyYrW1tPpm-24_zdtW227x9t48jlRqW8H5lz48mFDmgW6fXtM_2VsjGDV9lk5K3CFCNHW3dv9Np7VzTjLW2HQJv21LqFftf4Xkrlj04&amp;source=gmail&amp;ust=1763252898376000&amp;usg=AOvVaw1Er7hFAwxeqwu5sjEu8ZK1">Hunyuan3D-Omni</a>, a model that takes an image and rough 3D representation (such as a skeleton or bounding box) and generates a detailed 3D representation.&nbsp;</li>
<li aria-level="1">It also played a role in the release of <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCd_W3qgz0W7lCdLW6lZ3l7W8dckHh4n_TvlW1Hfh_N4KQh4HW1lHkcN41Dhb5N4Y-q5rSLzMtW1XTb2M6FHnP5W5Tq6VN72zmgqW9hhqvs15lCqZW8WXFFG87wbMGN3GGMwpBZJD_W5W2hn64Q0-BvW7NKh9X26njT0N8Ql8P1b9sDsW7RXV8X7ZpgdyW2DGQQP4BXVxJW6RdrXB4xyChxW3QfBnD5b61VXN5zzxY_CJdwhW5b348B7QzvQrW88x41h147GMYW7Fwjw88YKLksVPYFd86-jWXxW84bTph3g9hnqW6XLsm47snzz1V-Zlrp48PRCxf3QKNH404" rel="noopener" style="color:#237b94" target="_blank" data-saferedirecturl="https://www.google.com/url?q=https://info.deeplearning.ai/e3t/Ctc/LX%2B113/cJhC404/MXgR_cJNzG9W4pQdl63bzKdnW8QwHkG5FRFQ5N7lCd_W3qgz0W7lCdLW6lZ3l7W8dckHh4n_TvlW1Hfh_N4KQh4HW1lHkcN41Dhb5N4Y-q5rSLzMtW1XTb2M6FHnP5W5Tq6VN72zmgqW9hhqvs15lCqZW8WXFFG87wbMGN3GGMwpBZJD_W5W2hn64Q0-BvW7NKh9X26njT0N8Ql8P1b9sDsW7RXV8X7ZpgdyW2DGQQP4BXVxJW6RdrXB4xyChxW3QfBnD5b61VXN5zzxY_CJdwhW5b348B7QzvQrW88x41h147GMYW7Fwjw88YKLksVPYFd86-jWXxW84bTph3g9hnqW6XLsm47snzz1V-Zlrp48PRCxf3QKNH404&amp;source=gmail&amp;ust=1763252898376000&amp;usg=AOvVaw17FaI4mOVuvGwMHVRh8Td9">FlashWorld</a>, which accepts an image and text prompt and generates a 3D scene.</li>
</ul>
<p style="line-height:150%"><span style="font-weight:bold">Why it matters:</span> Simplifying training methods can be helpful, since each additional step adds time spent not only training but also debugging, and each additional component can interact with other components in unexpected ways, which adds to the time required to debug the system. Yet Tencent used several stages of pretraining and fine-tuning&nbsp;and produced a superior model.</p>
<p style="line-height:150%">&nbsp;</p>
<p style="line-height:150%"><span style="font-weight:bold">We’re thinking:</span> One key to this success may be to use&nbsp;different methods for different purposes. For instance, the team used MixGRPO to fine-tune the model for aesthetics and SRPO to better match human preferences.</p>
<p style="line-height:150%">&nbsp;</p></div>

<span style="opacity: 0;">Tags: Technology,Artificial Intelligence,Large Language Models,</span>